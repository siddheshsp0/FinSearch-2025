{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7010a10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 21:57:12.172550: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-13 21:57:12.173350: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-13 21:57:12.177604: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-13 21:57:12.189668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752424032.210137   40481 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752424032.215909   40481 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752424032.231365   40481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752424032.231388   40481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752424032.231390   40481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752424032.231391   40481 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-13 21:57:12.237127: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8baa99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    '''Handles replay buffers'''\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size # Max size of the buffer\n",
    "        self.mem_cntr = 0 # Counter to track number of elements in the buffer\n",
    "        self.input_shape = input_shape\n",
    "        # Initialising the tuple (state, action, reward, next_state, done), but in form of different \n",
    "            # numpy arrays\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape)) # * is the unpacking operator\n",
    "        self.next_state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool) # stores if the experience was stored at the end of an episode\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        '''Store new experience'''\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.next_state_memory[index] = next_state\n",
    "        self.terminal_memory[index] = done\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        next_states = self.next_state_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f76a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(keras.Model):\n",
    "    '''Critic network (Q network)'''\n",
    "    def __init__(self, fc1_dim, fc2_dim, name, chkpt_dir='tmp/td3'):\n",
    "        '''fc1 and fc2 dim is dimensions of 2 fully connected layers'''\n",
    "        super().__init__()\n",
    "        # Initialisation\n",
    "        self.fc1_dim = fc1_dim\n",
    "        self.fc2_dim = fc2_dim \n",
    "        self.model_name = name\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, self.model_name+'_td3'+'.weights.h5')\n",
    "\n",
    "        # constructing the model, 2 fully connected layers and an output\n",
    "            # We dont need to specify input dimensions, it infers automatically (nice)\n",
    "        self.fc1 = Dense(self.fc1_dim, activation='relu')\n",
    "        self.fc2 = Dense(self.fc2_dim, activation='relu')\n",
    "        self.q = Dense(1, activation=None) # Output (Q value)\n",
    "\n",
    "    def call(self, state, action): # keras.Model function which is called when the model is called like a function\n",
    "        q1_action_value = self.fc1(tf.concat([state, action], axis=1))\n",
    "        q1_action_value = self.fc2(q1_action_value)\n",
    "\n",
    "        q = self.q(q1_action_value)\n",
    "\n",
    "        return q\n",
    "\n",
    "\n",
    "class ActorNetwork(keras.Model):\n",
    "    def __init__(self, fc1_dim, fc2_dim, n_actions, name, chkpt_dir='tmp/td3'):\n",
    "        '''fc1 and fc2 dim is dimensions of 2 fully connected layers'''\n",
    "        super().__init__()\n",
    "        # Initialisation\n",
    "        self.fc1_dim = fc1_dim\n",
    "        self.fc2_dim = fc2_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.model_name = name\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, self.model_name+'_td3'+'.weights.h5')\n",
    "\n",
    "        self.fc1 = Dense(self.fc1_dim, activation='relu')\n",
    "        self.fc2 = Dense(self.fc2_dim, activation='relu')\n",
    "        self.action = Dense(self.n_actions, activation='tanh') # tanh activations coz\n",
    "        # tanh function limits the output between -1 and 1, so we can set limits by multiplying this value\n",
    "        # if needed. For example if my action is between -2 and 2, I'll multiply my action given by the \n",
    "        # model by 2\n",
    "\n",
    "    def call(self, state):\n",
    "        vals = self.fc1(state)\n",
    "        vals = self.fc2(vals)\n",
    "\n",
    "        action = self.action(vals)\n",
    "        return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9cac1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The real stuff starts here\n",
    "class Agent:\n",
    "    def __init__(self, alpha, beta, input_dims, tau, env, gamma=0.99, update_actor_interval=2,\n",
    "                 warmup=1000, n_actions=2, max_size=1000000, layer1_size=400, layer2_size=300,\n",
    "                 batch_size=300, noise=0.1):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.max_action = env.action_space.high[0]\n",
    "        self.min_action = env.action_space.low[0]\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "        self.learn_step_cntr = 0 # for delaying the actor training so that critic can converge\n",
    "        self.time_step = 0 # for the warmup procedure \n",
    "        self.warmup = warmup # initial exploration time\n",
    "        self.n_actions = n_actions\n",
    "        self.update_actor_iter = update_actor_interval # interval at which actor is updated\n",
    "\n",
    "        # Initialising networks\n",
    "        self.actor = ActorNetwork(layer1_size, layer2_size, self.n_actions, name='actor')\n",
    "        self.critic_1 = CriticNetwork(layer1_size, layer2_size, name='critic_1')\n",
    "        self.critic_2 = CriticNetwork(layer1_size, layer2_size, name='critic_2')\n",
    "\n",
    "        self.target_actor = ActorNetwork(layer1_size, layer2_size, self.n_actions, name='target_actor')\n",
    "        self.target_critic_1 = CriticNetwork(layer1_size, layer2_size, name='target_critic_1')\n",
    "        self.target_critic_2 = CriticNetwork(layer1_size, layer2_size, name='target_critic_2')\n",
    "\n",
    "        self.actor.compile(optimizer=Adam(learning_rate=alpha))\n",
    "        self.critic_1.compile(optimizer=Adam(learning_rate=beta), loss='mean_squared_error')\n",
    "        self.critic_2.compile(optimizer=Adam(learning_rate=beta), loss='mean_squared_error')\n",
    "        self.target_actor.compile(optimizer=Adam(learning_rate=alpha), loss='mean')\n",
    "        self.target_critic_1.compile(optimizer=Adam(learning_rate=beta), loss='mean_squared_error')\n",
    "        self.target_critic_2.compile(optimizer=Adam(learning_rate=beta), loss='mean_squared_error')\n",
    "\n",
    "        self.noise = noise\n",
    "        self.update_network_parameters(tau=1) # set target parameters = online network parameters\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        '''Choose an action for given observation'''\n",
    "        # Check if warmup period is going on\n",
    "        if self.time_step < self.warmup:\n",
    "            action = np.random.normal(scale=self.noise, size=(self.n_actions,))\n",
    "        else: # else, if warmup period is over\n",
    "            state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
    "            action = self.actor(state)[0] # Pass state through actor network, and recieve actions\n",
    "        action_prime = action + np.random.normal(scale=self.noise) # Add some exploration noise to our action\n",
    "        action_prime = tf.clip_by_value(action_prime, self.min_action, self.max_action) # Clip the action to allowed value\n",
    "        self.time_step+=1\n",
    "        return action_prime\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        '''Store an experience'''\n",
    "        self.memory.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        '''Function which makes the model learn'''\n",
    "        # We don't want to update parameters if atleast batch_size number of experiences are not stored,\n",
    "        # else it does not make any sense to sample\n",
    "        if (self.memory.mem_cntr < self.batch_size):\n",
    "            return\n",
    "        \n",
    "        # Now we sample from our buffer for training\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample_buffer(self.batch_size)\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "\n",
    "\n",
    "        # # # For Critic Network\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Passing s_t+1 in the actor and computing target actions for s_t+1\n",
    "                # By adding noise and then clipping it, TD3 forces the critic to learn to be robust to\n",
    "                # small perturbations in action.\n",
    "            noise = tf.clip_by_value(tf.random.normal(shape=[self.batch_size, self.n_actions], stddev=0.2), -0.5, 0.5)\n",
    "            target_actions = self.target_actor(next_states) + noise\n",
    "            target_actions = tf.clip_by_value(target_actions, self.min_action, self.max_action)\n",
    "\n",
    "\n",
    "            # Get target q values for s_t+1 and target actions for s_t+1\n",
    "                # Shape is [batch_size, 1], convert it to [batch_size]\n",
    "            q1_ = tf.squeeze(self.target_critic_1(next_states, target_actions), 1)\n",
    "            q2_ = tf.squeeze(self.target_critic_2(next_states, target_actions), 1)\n",
    "\n",
    "            # Get q values for s_t\n",
    "            q2 = tf.squeeze(self.critic_2(states, actions), 1)\n",
    "            q1 = tf.squeeze(self.critic_1(states, actions), 1)\n",
    "\n",
    "            # Get min target q value\n",
    "            critic_value_ = tf.math.minimum(q1_, q2_)\n",
    "\n",
    "            # Computing yt (from the flowchart)\n",
    "            dones = tf.cast(dones, dtype=tf.float32)\n",
    "            target = rewards + self.gamma * critic_value_ * (1-dones)\n",
    "\n",
    "            # Computing losses for the critic\n",
    "            critic_1_loss = keras.losses.mean_squared_error(target, q1)\n",
    "            critic_2_loss = keras.losses.mean_squared_error(target, q2)\n",
    "        \n",
    "        # Gradient descent !\n",
    "        critic_1_gradient = tape.gradient(critic_1_loss, self.critic_1.trainable_variables)\n",
    "        critic_2_gradient = tape.gradient(critic_2_loss, self.critic_2.trainable_variables)\n",
    "\n",
    "        self.critic_1.optimizer.apply_gradients(zip(critic_1_gradient, self.critic_1.trainable_variables))\n",
    "        self.critic_2.optimizer.apply_gradients(zip(critic_2_gradient, self.critic_2.trainable_variables))\n",
    "\n",
    "        # Learn step counter update\n",
    "        self.learn_step_cntr+=1\n",
    "\n",
    "        if (self.learn_step_cntr % self.update_actor_iter != 0):\n",
    "            return\n",
    "        # # # For actor network\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_actions = self.actor(states)\n",
    "            critic_1_value = self.critic_1(states, new_actions)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_1_value)\n",
    "\n",
    "        # Chain rule: differentiating q value wrt actor's parameters\n",
    "        actor_gradient = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_gradient, self.actor.trainable_variables))\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        \n",
    "        # Soft update target weights (actor, critic1, critic2)\n",
    "        weights = []\n",
    "        targets = self.target_actor.weights\n",
    "        for i, weight in enumerate(self.actor.weights):\n",
    "            weights.append(weight * tau + targets[i] * (1 - tau))\n",
    "        self.target_actor.set_weights(weights=weights)\n",
    "        \n",
    "        weights = []\n",
    "        targets = self.target_critic_1.weights\n",
    "        for i, weight in enumerate(self.critic_1.weights):\n",
    "            weights.append(weight * tau + targets[i] * (1 - tau))\n",
    "        self.target_critic_1.set_weights(weights=weights)\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_critic_2.weights\n",
    "        for i, weight in enumerate(self.critic_2.weights):\n",
    "            weights.append(weight * tau + targets[i] * (1 - tau))\n",
    "        self.target_critic_2.set_weights(weights=weights)\n",
    "\n",
    "    def save_model(self):\n",
    "        print(\"Saving model...\")\n",
    "        self.actor.save_weights(self.actor.checkpoint_file)\n",
    "        self.critic_1.save_weights(self.critic_1.checkpoint_file)\n",
    "        self.critic_2.save_weights(self.critic_2.checkpoint_file)\n",
    "        self.target_actor.save_weights(self.target_actor.checkpoint_file)\n",
    "        self.target_critic_1.save_weights(self.target_critic_1.checkpoint_file)\n",
    "        self.target_critic_2.save_weights(self.target_critic_2.checkpoint_file)\n",
    "\n",
    "    def load_model(self):\n",
    "    # Build models by passing dummy input once\n",
    "        dummy_state = tf.convert_to_tensor(np.zeros((1, *self.memory.input_shape)), dtype=tf.float32)\n",
    "        dummy_action = tf.convert_to_tensor(np.zeros((1, self.n_actions)), dtype=tf.float32)\n",
    "\n",
    "        # Call once to initialize weights\n",
    "        self.actor(dummy_state)\n",
    "        self.critic_1(dummy_state, dummy_action)\n",
    "        self.critic_2(dummy_state, dummy_action)\n",
    "        self.target_actor(dummy_state)\n",
    "        self.target_critic_1(dummy_state, dummy_action)\n",
    "        self.target_critic_2(dummy_state, dummy_action)\n",
    "\n",
    "        # Now load weights\n",
    "        self.actor.load_weights(self.actor.checkpoint_file)\n",
    "        self.critic_1.load_weights(self.critic_1.checkpoint_file)\n",
    "        self.critic_2.load_weights(self.critic_2.checkpoint_file)\n",
    "        self.target_actor.load_weights(self.target_actor.checkpoint_file)\n",
    "        self.target_critic_1.load_weights(self.target_critic_1.checkpoint_file)\n",
    "        self.target_critic_2.load_weights(self.target_critic_2.checkpoint_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faff0f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import gymnasium as gym\n",
    "\n",
    "# # Create environment\n",
    "# env = gym.make(\"Pendulum-v1\", render_mode=None)\n",
    "\n",
    "# # Create agent\n",
    "# agent = Agent(alpha=0.001, beta=0.001, input_dims=env.observation_space.shape,\n",
    "#               tau=0.005, env=env, batch_size=100, n_actions=env.action_space.shape[0])\n",
    "\n",
    "# n_episodes = 100  # Increase for solving\n",
    "# max_steps = 200\n",
    "# scores = []\n",
    "# moving_avg_scores = []\n",
    "\n",
    "# for episode in range(n_episodes):\n",
    "#     observation, _ = env.reset()\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "\n",
    "#     for step in range(max_steps):\n",
    "#         action = agent.choose_action(observation)\n",
    "#         next_observation, reward, terminated, truncated, _ = env.step(action.numpy())\n",
    "#         done = terminated or truncated\n",
    "\n",
    "#         agent.remember(observation, action.numpy(), reward, next_observation, done)\n",
    "#         agent.learn()\n",
    "#         observation = next_observation\n",
    "#         total_reward += reward\n",
    "\n",
    "#         if done:\n",
    "#             break\n",
    "\n",
    "#     scores.append(total_reward)\n",
    "#     moving_avg = np.mean(scores[-10:])\n",
    "#     moving_avg_scores.append(moving_avg)\n",
    "#     print(f\"Episode {episode+1}: Score = {total_reward:.2f}, Moving Avg (last 10) = {moving_avg:.2f}\")\n",
    "\n",
    "# # Save trained models\n",
    "# agent.save_model()\n",
    "\n",
    "# # Plot total rewards\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(scores, label='Episode Rewards')\n",
    "# plt.plot(moving_avg_scores, label='Moving Average (10 episodes)')\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Total Reward\")\n",
    "# plt.title(\"TD3 on Pendulum-v1\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd25319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1752424034.884404   40481 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1752424034.885834   40481 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/siddheshsp0/anaconda3/envs/tf_env/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward during test run: -969.30\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"Pendulum-v1\", render_mode='human')\n",
    "observation, _ = env.reset()\n",
    "\n",
    "# Optional: Load saved model\n",
    "agent = Agent(alpha=0.001, beta=0.001, input_dims=env.observation_space.shape,\n",
    "              tau=0.005, env=env, batch_size=100, n_actions=env.action_space.shape[0])\n",
    "agent.noise = 0.0\n",
    "agent.load_model()\n",
    "\n",
    "total_reward = 0\n",
    "for step in range(200):\n",
    "    # Use the actor network directly (no noise)\n",
    "    state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
    "    action = agent.actor(state)[0].numpy()\n",
    "\n",
    "    observation, reward, terminated, truncated, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "print(f\"Total reward during test run: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebe847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
